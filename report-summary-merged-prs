#! /usr/bin/env python

from optparse import OptionParser
import subprocess
import re
import json
import pickle
from pickle import Unpickler
from gitmergesgraph import *
from os.path import basename, dirname, exists, join
from glob import glob
#-----------------------------------------------------------------------------------
#---- Parser Options
#-----------------------------------------------------------------------------------
parser = OptionParser(usage="usage: %prog CMSSW_REPO GITHUB_IO_REPO START_DATE ARTIFACTS_MACHINE"
                            "\n CMSSW_REPO: location of the cmssw repository. This must be a bare clone ( git clone --bare )"
                            "\n CMSDIST_REPO: location of the cmsdist repository. This must be a normal clone"
                            "\n GITHUB_IO_REPO: location of the github.io repository. This must be a normal clone"
			    "\n for example: cmssw.git or /afs/cern.ch/cms/git-cmssw-mirror/cmssw.git"
                            "\n START_DATE: the date of the earliest IB to show. It must be in the format"
			    "\n <year>-<day>-<month>-<hour>"
                            "\n For example:"
			    "\n 2014-10-08-1400"
                            "\n ARTIFACTS_MACHINE: the artifacts machine to use to look for static checks and hlt tests")

parser.add_option( "-v" , "--verbose" , dest="verbose" , action="store_true", help="Do not post on Github", default=False )

(options, args) = parser.parse_args()

#-----------------------------------------------------------------------------------
#---- Output Schema
#-----------------------------------------------------------------------------------
#
# comparisons": [ <DictA>, <DictB>, <DictC> ]
#
# Each dict contains the result of the comparison between 2 tags in cmssw. For example 
# CMSSW_5_3_X_2015-02-03-0200 with CMSSW_5_3_X_2015-02-04-0200 which correspond 
# to the IB CMSSW_5_3_X_2015-02-04-0200
#
# The schema of the dictionary is as folows:
# {
#    "addons": [],
#    "builds": [],
#    "compared_tags": "",
#    "utests": [],
#    "cmsdistTags": {},
#    "relvals": [],
#    "static_checks": "",
#    "isIB": Boolean,
#    "tests_archs": [],
#    "release_name": "",
#    "merged_prs": [],
#    "RVExceptions" : Boolean
#
# }

#-----------------------------------------------------------------------------------
#---- Review of arguments
#-----------------------------------------------------------------------------------

if (len(args)<5):
  print 'not enough arguments\n'
  parser.print_help()
  exit()

# Remember that the cmssw repo is a bare clone while cmsdist is a complete clone
CMSSW_REPO = args[ 0 ]
GITHUB_IO_REPO = args[ 1 ]
CMSDIST_REPO = args[ 2 ]
START_DATE = args[ 3 ]
ARTIFACTS_MACHINE = args[ 4 ]

#-----------------------------------------------------------------------------------
#---- Fuctions
#-----------------------------------------------------------------------------------

#
# Takes into account the verbose option. If the option is activated it douesn't print anything. 
#
def print_verbose( msg ):
  if options.verbose:
    print ( msg )

# reads a line of config.map and returns a dictionary with is parameters
def parse_config_map_line ( line ):
  params = {}
  parts = line.split( ';' )

  for part in parts:
    if part == '':
      continue
    key = part.split( '=' )[ 0 ]
    value = part.split( '=' )[ 1 ]
    params[ key ] = value

  return params

# gets the list of architectures by reading config.map, they are saved in ARCHITECTURES
# gets the releases branches from config.map, they are saved in RELEASES_BRANCHES
# it maps the branches for all the releases this is to take into account the case in which the base branch
# is different from the release queue
def get_config_map_params():

  f = open( CONFIG_MAP_FILE , 'r' )
  for line in f.readlines():
    params = parse_config_map_line ( line.rstrip() )
    print params

    arch = params[ 'SCRAM_ARCH' ]
    if arch not in ARCHITECTURES:
      ARCHITECTURES.append( arch )

    release_queue = params[ 'RELEASE_QUEUE' ]
    base_branch = params.get( 'RELEASE_BRANCH' )
    if base_branch:
      RELEASES_BRANCHES[ release_queue ] = base_branch
    else:
      RELEASES_BRANCHES[ release_queue ] = release_queue

    sp_rel_name = release_queue.split( '_' )[ 3 ]

    if sp_rel_name != 'X' and sp_rel_name not in SPECIAL_RELEASES:
      SPECIAL_RELEASES.append( sp_rel_name )

    if not params.get( 'DISABLED' ):
      if not RELEASES_ARCHS.get( release_queue ):
        RELEASES_ARCHS[ release_queue ] = []
      RELEASES_ARCHS[ release_queue ].append( arch )
      if release_queue not in RELEASE_QUEUES:
        RELEASE_QUEUES.append( release_queue )
      

    additional_tests = params.get( 'ADDITIONAL_TESTS' )

    if additional_tests:
      if not RELEASE_ADITIONAL_TESTS.get( release_queue ):
        RELEASE_ADITIONAL_TESTS[ release_queue ] = {}
      RELEASE_ADITIONAL_TESTS[ release_queue ][ arch ] = [ test for test in additional_tests.split( ',' ) if test != 'baseline' and test != 'dqm' ]

  SP_REL_REGEX = "|".join(SPECIAL_RELEASES)
  RELEASE_QUEUES.sort()
 
  print
  print '---------------------------'
  print 'Read config.map:'
  print 'ARCHS:'
  print ARCHITECTURES
  print '--'
  print RELEASES_ARCHS
  print 'RELEASES_BRANCHES:'
  print RELEASES_BRANCHES
  print 'special releases'
  print SPECIAL_RELEASES
  print 'aditional tests'
  print RELEASE_ADITIONAL_TESTS
  print 'I am going to show:'
  print RELEASE_QUEUES
  print '---------------------------'
  print


# Tells if the pr comes from a merge commit it reads the first part of the line that
# was obtained from git log --graph
def is_pr_from_merge_commit(graph_part):
  return graph_part.startswith('|')

def get_pr_number(line_parts):
  number_and_name = line_parts[1].replace('"','')
  number = re.sub(' from .*', '', number_and_name)
  number = re.sub('Merge pull request #', '', number)
  return number

def get_pr_author_login(line_parts):
  number_and_name = line_parts[1].replace('"','')
  name = re.sub('^Merge .* from ', '', number_and_name)
  name = name.split('/')[0]
  return name

def get_pr_commit_hash(line_parts):
  hash = line_parts[0].replace('"','')
  return hash

def get_pr_title(line_parts):
  title = line_parts[2].replace('"','').strip()
  return title

# gets the information of the pull request
def get_info_pr( line , graph ):
  pull_request = {}
  line_parts = line.split(',')
  pull_request['hash'] = get_pr_commit_hash(line_parts)
  pull_request['number'] = get_pr_number(line_parts)
  pull_request['author_login'] = get_pr_author_login(line_parts)
  pull_request['title'] = get_pr_title(line_parts)
  pull_request['url'] = 'https://github.com/cms-sw/cmssw/pull/%d' % int(pull_request['number'])
  pull_request['from_merge_commit'] = graph[pull_request['hash']].is_from_merge
  pull_request['is_merge_commit'] = False
  return pull_request

def get_info_merge_commit( line , graph ):
  merge_commit = {}
  line_parts = line.split(',')
  merge_commit['hash'] = get_pr_commit_hash(line_parts)
  merge_commit['number'] = merge_commit['hash'][0:7]
  merge_commit['is_merge_commit'] = True
  merge_commit['from_merge_commit'] = False
  brought_commits = get_prs_brought_by_commit( graph , merge_commit['hash'] )
  merge_commit['brought_prs'] = [ pr.pr_number for pr in brought_commits ]

  return merge_commit

#reads a line of the output of git log and returns the tags that it contains
#if there are no tags it returns an empty list
#it applies filters according to the release queue to only get the
#tags related to the current release queue
def get_tags_from_line( line , release_queue):
  if 'tags->' not in line:
    return []
  tags_str = line.split('tags->')[1]
  if re.match( '.*SLHC$', release_queue):
    filter = release_queue[:-6]+'[X|0-9]_SLHC.*'
  else:
    filter = release_queue[:-1]+'[X|0-9].*'

  ## if the tags part is equal to ," there are no tags
  if tags_str != ',"':
    tags = tags_str.split(',',1)[1].strip().replace('(','').replace(')','').split(',')
    #remove te word "tag: "
    tags = [t.replace('tag: ','') for t in tags ]
    #I also have to remove the branch name because it otherwise will always appear
    #I also remove tags that have the  string _DEBUG_TEST, they are used to create test IBs
    tags = [ t for t in tags if re.match(filter,t.strip()) and ( t.strip().replace('"','') != release_queue ) and ( 'DEBUG_TEST' not in t ) ]
    return [ t.replace('"','').replace('tag:','').strip() for t in tags ]
  else:
    return []

# Returns all pull request found in a list of comparisons, it returns then in a
# dictionary in which the key is the pr number
def get_all_prs(comparisons):
  prs = {}
  for comp in comparisons:
    for pr in comp['merged_prs']:
      prs[pr['number']] = pr
  return prs

#-----------------------------------------------------------------------------------
#---- Fuctions -- Analize Git ouputs
#-----------------------------------------------------------------------------------

def determine_build_error(nErrorInfo):
  a = BuildResultsKeys.COMP_ERROR in nErrorInfo.keys()
  b = BuildResultsKeys.LINK_ERROR in nErrorInfo.keys()
  c = BuildResultsKeys.MISC_ERROR in nErrorInfo.keys()
  d = BuildResultsKeys.DWNL_ERROR in nErrorInfo.keys()
  e = BuildResultsKeys.DICT_ERROR in nErrorInfo.keys()
  f = BuildResultsKeys.PYTHON_ERROR in nErrorInfo.keys()
  return a or b or c or d or e or f

def determine_build_warning(nErrorInfo):
  return BuildResultsKeys.COMP_WARNING in nErrorInfo.keys()

def get_results_one_addOn_file(file):
  look_for_err_cmd = 'grep "failed" %s' %file
  result,err,ret_code = get_output_command(look_for_err_cmd)
  if '0 failed' in result:
    return True
  else:
    return False

#
# given a unitTests-summary.log it determines if the test passed or not
# it returns a tuple, the first element is one of the possible values of PossibleUnitTestResults
# The second element is a dictionary which indicates how many tests failed
#
def get_results_one_unitTests_file(file):
  look_for_err_cmd = 'grep -c "ERROR" %s' %file
  result,err,ret_code = get_output_command(look_for_err_cmd)

  result = result.rstrip()

  details = { 'num_fails':result }

  if result != '0':
    return PossibleUnitTestResults.FAILED,details
  else:
    return PossibleUnitTestResults.PASSED,details
#
# given a runall-report-step123-.log file it returns the result of the relvals
# it returns a tuple, the first element indicates if the tests passed or not
# the second element is a dictionary which shows the details of how many relvals pased
# and how meny failed
#
def get_results_one_relval_file(filename):
  details = { 'num_passed' : 0,
              'num_failed' : 1}

  print_verbose( 'Analyzing: ' + filename )
  lines = file(filename).read().split("\n")
  results = [x for x in lines if ' tests passed' in x]
  if len(results)==0:
    return False,details
  out = results.pop()

  num_passed_sep = out.split(',')[0].replace( ' tests passed' , '' ).strip()
  num_failed_sep = out.split(',')[1].replace( ' failed' , '' ).strip()
  try:
    details["num_passed"] = sum( [ int(num) for num in num_passed_sep.split(' ') ] )
    details["num_failed"] = sum( [ int(num) for num in num_failed_sep.split(' ') ] )
  except ValueError, e:
    print "Error while reading file %s" % filename
    print e
    return False, details

  if details["num_failed"] == 0:
    return True,details
  else:
    return False,details

#
# Given a logAnalysis.pkl file, it determines if the tests passed or not
# it returns a tuple, the first element is one of the values of PossibleBuildResults
# The second element is a dictionary contaning the details of the results.
# If the tests are all ok this dictionary is empty
#
def get_results_details_one_build_file(file):
  summFile = open(file,'r')
  pklr = Unpickler(summFile)
  [rel, plat, anaTime]   = pklr.load()
  errorKeys   = pklr.load()
  nErrorInfo  = pklr.load()
  summFile.close()

  if determine_build_error(nErrorInfo):
    return PossibleBuildResults.ERROR,nErrorInfo
  elif determine_build_warning(nErrorInfo):
    return PossibleBuildResults.WARNING,nErrorInfo
  else:
    return PossibleBuildResults.PASSED,nErrorInfo

  return True,nErrorInfo


#
# parses the tests results for each file in output. It distinguishes if it is
# build, unit tests, relvals, or addon tests logs. The the result of the parsing
# is saved in the parameter results.
# type can be 'relvals', 'utests', 'addON', 'builds'
#
# schema of results:
# {
#   "<IBName>": [ result_arch1, result_arch2, ... result_archN ]
# }
# schema of result_arch
# {
#   "arch"    : "<architecture>"
#   "file"    : "<location of the result>"
#   "passed"  : <true or false> ( if not applicable the value is true )
#   "details" : <details for the tests> ( can be empty if not applicable, but not undefined )
# }
#
def analyze_tests_results( output, results, arch, type ):
  for line in output.splitlines():
    m = re.search('CMSSW.*[0-9]/', line)
    if not m:
      print_verbose( 'Ignoring file:\n%s' % line )
      continue

    print "Processing ",type,":",line
    rel_name = line[m.start():m.end()-1]
    result_arch = {}
    result_arch['arch'] = arch
    result_arch['file'] = line

    details = {}
    if type == 'relvals':
      passed,details = get_results_one_relval_file(line)
      result_arch['done']=False
      if exists(join(dirname(line),"done")):
        result_arch['done']=True
    elif type == 'utests':
      passed,details = get_results_one_unitTests_file(line)
    elif type == 'addOn':
      passed = get_results_one_addOn_file(line)
    elif type == 'builds':
      passed,details = get_results_details_one_build_file(line)
    else:
      print 'not a valid test type %s' %type
      exit(1)

    result_arch['passed'] = passed
    result_arch['details'] = details

    if rel_name not in results.keys():
      results[rel_name] = []

    results[rel_name].append(result_arch)


#
# Searchs in github.io for the results for relvals exceptions
#
def execute_magic_command_find_rv_exceptions_results( ):
  print ( 'Finding relval exceptions results...')
  command_to_execute = MAGIC_COMMAND_FIND_EXCEPTIONS_RESULTS_RELVALS
  out,err,ret_code = get_output_command(command_to_execute)

  rv_exception_results = {}

  for line in out.splitlines():
    line_parts = line.split( '/' )
    ib_name = line_parts[ -1 ].replace( 'EXCEPTIONS.json', '' ) + line_parts[ -2 ]
    rv_exception_results[ ib_name ] = True 

  return rv_exception_results

#
# Searchs in github.io for incomplete results. It fills a structure with the results found
# this is done to discard tags at the begining
#
def fill_list_incomplete_results( type ):
  print( 'Getting incomplete ', type, ' results...')
  dict = {}
  if type == 'relvals':
    command_to_execute = MAGIC_COMMAD_FIND_INCOMPLETE_RESULTS_RELVALS
    dict = INCOMPLETE_RELVALS_RESULTS
  elif type == 'builds':
    command_to_execute = MAGIC_COMMAD_FIND_INCOMPLETE_RESULTS_BUILD
    dict = INCOMPLETE_BUILD_RESULTS
  else:
    print( "I don't know this type of result", type )
    exit( 0 )

  out,err,ret_code = get_output_command(command_to_execute)

  for line in out.splitlines():
    line_parts = line.split( '/' )
    ib_name = line_parts[ -1 ].replace( 'INCOMPLETE.json', '' ) + line_parts[ -2 ]
    arch = line_parts[ -3 ]

    if not dict.get( ib_name ):
      dict[ ib_name ] = []
    dict[ ib_name ].append( arch )

  print_verbose( type )
  print_verbose( dict )


#
# Searchs in github.io for incomplete results. 
# It looks into the existing results and adds them to the structure.
#
def execute_magic_command_find_incomplete_results( results, type ):

  print( 'Adding incomplete results...' )
  dict = {}
  if type == 'relvals':
    dict = INCOMPLETE_RELVALS_RESULTS
  elif type == 'builds':
    dict = INCOMPLETE_BUILD_RESULTS
  else: 
    print( "I don't know this type of result", type )
    exit( 0 )

  for ib_name in dict.keys():
    for arch in dict[ ib_name ]:
      result_arch = { "arch" : arch,
                    "passed" : True,
                    "file" : "not-ready",
                    "details" : {}
                  }
      if not results.get( ib_name ):
        results[ ib_name ] = []
   



      # if there is a result for that arch, I don't add the incomplete one
      if not [ r['arch'] for r in results[ ib_name ] if r['arch'] == arch ]:
        results[ ib_name ].append( result_arch )
        print_verbose( 'Added incomplete: ' + type + ', ' + arch +', ' + ib_name )
      else:
        print_verbose( 'NOT Added incomplete: ' + type + ', ' + arch +', ' + ib_name )


# returns a list of tags based on git log output
# It uses the release queue name to filter the tags, this avoids having
# in the result tags from other queues that may come from automatic merges.
# For example, if release_queue is 7_2_X, it will drop tags like CMSSW_7_2_THREADED_X_2014-09-15-0200
def get_tags( git_log_output , release_queue ):
  tags = []
  for line in git_log_output.splitlines():
    tags += get_tags_from_line(line, release_queue)

  if (len(tags) ==0):
    print "ATTENTION:"
    print "looks like %s has not changed between the tags specified!" % release_queue
    command_to_execute = MAGIC_COMMAND_FIND_FIRST_MERGE_WITH_TAG.replace('END_TAG',release_queue)
    out,err,ret_code = get_output_command(command_to_execute)
    print out
    tags = get_tags_from_line(out, release_queue)
    print tags

  return tags

#returns the number of the day of a tag
#if it is not an IB tag, it returns -1
def get_day_number_tag(tag):
  parts = tag.split("-")
  if len(parts) == 1:
    return -1
  else:
    day = parts[2]
    try:
        return int(day)
    except ValueError:
        return -1


# uses some heuristics to tell if the list of tags seems to be too short
def is_tag_list_suspicious(tags):
  if len(tags) < 7:
    return True
  day_first_tag = get_day_number_tag(tags[-1])
  day_second_tag = get_day_number_tag(tags[-2])
  return day_second_tag-day_first_tag > 1

# determines if  the error is because one of the tags does not exist
# this can happen when the branch that is being analyzed has been
# created recently
def is_recent_branch(err):
  return "unknown revision or path not in the working tree" in err

#-----------------------------------------------------------------------------------
#---- Fuctions -- Execute Magic commands
#-----------------------------------------------------------------------------------

# this calls the git log command with the first tag to look for missing
# tags that were not found previously
def look_for_missing_tags(start_tag, release_queue):
  command_to_execute = MAGIC_COMMAND_FIND_FIRST_MERGE_WITH_TAG.replace('END_TAG',start_tag)
  out,err,ret_code = get_output_command(command_to_execute)
  tags = get_tags_from_line(out, release_queue)
  return tags

# Executes the command that is given as parameter, returns a tuple out,err,ret_code
# with the output, error and return code obtained
def get_output_command(command_to_execute):
  print_verbose( 'Executing:' )
  print_verbose( command_to_execute )

  p = subprocess.Popen(command_to_execute, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
  out,err = p.communicate()
  ret_code = p.returncode

  if ret_code != 0:
    print_verbose( ret_code )
    print_verbose( 'Error:' )
    print_verbose( err )

  return out,err,ret_code

# Gets the tags between start_tag and end_tag, the release_queue is used as a filter
# to ignore tags that are from other releases
def execute_magic_command_tags( start_tag, end_tag, release_queue, release_branch ):

  print_verbose( 'Release Queue:' )
  print_verbose( release_queue )
  print_verbose( 'Release Branch:' )
  print_verbose( release_branch )

  # if it is a special release queue based on a branch with a different name, I use the release_branch as end tag
  if release_queue == release_branch:
    print_verbose( 'These IBs have a custom release branch' )
    real_end_tag = end_tag
  else:
    real_end_tag = release_branch

  print_verbose( 'Start tag:' )
  print_verbose( start_tag )
  print_verbose( 'End tag:' )
  print_verbose( real_end_tag )
  tags = []
  command_to_execute = MAGIC_COMMAND_TAGS.replace( 'START_TAG', start_tag ).replace( 'END_TAG', real_end_tag )
  out,err,ret_code = get_output_command(command_to_execute)

  # check if the end_tag exists, but the start_tag doesn't
  # this could mean that the release branch has been created recently
  if ret_code != 0:
    if is_recent_branch(err):
      print_verbose( 'looks like this branch has been created recently' )
      command_to_execute = MAGIC_COMMAND_FIND_ALL_TAGS.replace( 'END_TAG', real_end_tag ).replace( 'RELEASE_QUEUE', release_queue )
      out,err,ret_code = get_output_command(command_to_execute)

  tags = get_tags(out,release_queue)
  tags.append(start_tag)

  #check if the tags list could be missing tags
  # this means that the release branch has not changed much from the start_tag
  if is_tag_list_suspicious(tags):
    print_verbose( 'this list could be missing something!' )
    print_verbose( tags )
    new_tags = look_for_missing_tags(start_tag, release_branch)
    tags.pop()
    tags += new_tags

  tags = [t for t in reversed(tags)]

  return tags

def execute_command_compare_tags(start_tag,end_tag , graph ):
  command_to_execute = MAGIC_COMMAND_PRS.replace('START_TAG',start_tag).replace('END_TAG',end_tag)
  output,err,ret_code = get_output_command(command_to_execute)
  comp = {}
  comp['compared_tags'] = '%s-->%s' % (start_tag,end_tag)
  comp[ 'release_name' ] = end_tag
  prs = []
  for line in output.splitlines():
    # show in the list merge commits from forward port and pull requests.
    if re.match(AUTO_FORWARD_PORT_REGEX, line ):
      pr = get_info_merge_commit( line , graph )
      if pr['brought_prs'] == []:
        continue
    elif 'Merge pull ' in line:
      pr = get_info_pr( line , graph )
    else:
      continue

    prs.append(pr)
  comp['merged_prs'] = prs
  return comp

def compare_tags( tags , graph ):
  comparisons = []
  for i in range(len(tags)-1):
    comp = execute_command_compare_tags(tags[i],tags[i+1] , graph )
    comparisons.append(comp)
  return comparisons

#
# Executes the command to get the tags
# schema of all_tags_found:
# {
#   "<IBName>": {
#                 "<arch_name>" : "<tag_name>"
#               }
# }
def execute_magic_command_get_cmsdist_tags():
  all_tags_found = {}
  for arch in ARCHITECTURES:
    command_to_execute = MAGIC_COMMAND_CMSDIST_TAGS.replace( 'ARCHITECTURE' , arch )
    out,err,ret_code = get_output_command( command_to_execute )

    for line in out.splitlines():
      m = re.search('CMSSW.*[0-9]/', line)
      if not m:
        print 'Ignoring line:\n%s' % line
        continue

      rel_name = line[m.start():m.end()-1]

      if not all_tags_found.get( rel_name ):
        all_tags_found[ rel_name ] = {}

      all_tags_found[ rel_name ][ arch ] = line

  print 'ALL TAGS'
  print all_tags_found
  return all_tags_found


#
# Executes the a command to get the results for the relvals, unit tests,
# addon tests, and compitlation tests
# It saves the results in the parameter 'results'
# type can be 'relvals', 'utests', 'addON', 'builds'
#
def execute_magic_command_find_results( results, type ):
  for arch in ARCHITECTURES:
    if type == 'relvals':
      base_command = MAGIC_COMMAD_FIND_RESULTS_RELVALS
    elif type == 'utests':
      base_command = MAGIC_COMMAND_FIND_RESULTS_UNIT_TESTS
    elif type == 'addOn':
      base_command = MAGIC_COMMAND_FIND_RESULTS_ADDON
    elif type == 'builds':
      base_command = MAGIC_COMMAND_FIND_RESULTS_BUILD
    else:
      print 'not a valid test type %s' %type
      exit(1)

    command_to_execute = base_command.replace( 'ARCHITECTURE' , arch )
    print "Run>>",command_to_execute
    out,err,ret_code = get_output_command( command_to_execute )
    analyze_tests_results( out , results , arch , type )


def print_results(results):
  print "Results:"
  print
  print
  for rq in results:
    print
    print rq['release_name']
    print '/////////////////////////'
    for comp in rq['comparisons']:
      print comp['compared_tags']

      print '\t' + 'HLT Tests: ' + comp['hlt_tests']

      print '\t' + 'Static Checks: ' + comp['static_checks']

      cmsdist_tags = comp['cmsdistTags']
      print '\t' + 'cmsdist Tags:'+ str( cmsdist_tags )

      builds_results = [res['arch']+':'+str(res['passed'])+':'+str(res['details']) for res in comp['builds'] ]
      print '\t' + 'Builds:'+ str(builds_results)

      relvals_results = [res['arch']+':'+str(res['passed'])+":"+str(res['details']) for res in comp['relvals'] ]
      print '\t' + 'RelVals:'+ str(relvals_results)

      utests_results = [res['arch']+':'+str(res['passed']) +':'+str(res['details']) for res in comp['utests'] ]
      print '\t' + 'UnitTests:' + str(utests_results)

      addons_results = [res['arch']+':'+str(res['passed']) for res in comp['addons'] ]
      print '\t' + 'AddOns:' + str(addons_results)

      merged_prs = [pr['number'] for pr in comp['merged_prs']]
      print '\t' + 'PRs:' + str(merged_prs)

      from_merge_commit = [pr['number'] for pr in comp['merged_prs'] if pr['from_merge_commit']]
      print '\t' + 'From merge commit' + str(from_merge_commit)

      print '\t' + 'RVExceptions: ' + str( comp.get( 'RVExceptions' ) )
      print '\t' + 'inProgress: ' + str( comp.get( 'inProgress' ) )


#
# Iterates over the IBs comparisons, if an IB doesn't have a tag for an architecture, the previous tag is
# assigned. For example, for arch slc6_amd64_gcc481
# 1. CMSSW_7_1_X_2014-10-02-1500 was built using the tag IB/CMSSW_7_1_X_2014-10-02-1500/slc6_amd64_gcc481
# 2. There is no tag for CMSSW_7_1_X_2014-10-03-0200 in cmsdist
#
# Then, it assumes that the tag used for CMSSW_7_1_X_2014-10-03-0200 was IB/CMSSW_7_1_X_2014-10-02-1500/slc6_amd64_gcc481
#
def fill_missing_cmsdist_tags( results ):

  for rq in results:
    previous_cmsdist_tags = {}
    for comp in rq['comparisons']:
      rel_name = comp['compared_tags'].split('-->')[1]

      for arch in comp['tests_archs']:
        current_ib_tag_arch = comp[ 'cmsdistTags' ].get( arch )
        if current_ib_tag_arch:
          previous_cmsdist_tags[ arch ] = current_ib_tag_arch
        else:
          if previous_cmsdist_tags.get( arch ):
            comp[ 'cmsdistTags' ][ arch ] = previous_cmsdist_tags[ arch ]
          else:
            comp[ 'cmsdistTags' ][ arch ] = 'Not Found'

#
# merges the results of the tests with the structure of the IBs tags and the pull requests
# it also marks the comparisons that correspond to an IB
#
def add_tests_to_results( results, unit_tests, relvals_results, 
                          addon_results , build_results, cmsdist_tags_results, 
                          rv_Exceptions_Results ):
  for rq in results:
    for comp in rq['comparisons']:
      rel_name = comp['compared_tags'].split('-->')[1]
      rvsres = relvals_results.get(rel_name)
      utres = unit_tests.get(rel_name)
      adonres = addon_results.get(rel_name)
      buildsres = build_results.get(rel_name)
      cmsdist_tags = cmsdist_tags_results.get(rel_name)

      comp[ 'relvals' ] = rvsres if rvsres else []
      comp[ 'utests' ] = utres if utres else []
      comp[ 'addons' ] = adonres if adonres else []
      comp[ 'builds' ] = buildsres if buildsres else []
      comp[ 'cmsdistTags' ] = cmsdist_tags if cmsdist_tags else {}
      comp[ 'isIB' ] = '-' in rel_name
      comp[ 'RVExceptions' ] = rv_Exceptions_Results.get( rel_name )
      
      comp[ 'inProgress' ] = rel_name in ALL_BUILDING_IBS

      if not comp.get( 'static_checks' ):
        comp[ 'static_checks' ] = 'not-found'
      if not comp.get( 'hlt_tests' ):
        comp['hlt_tests' ] = 'not-found'

      a = [t['arch'] for t in utres] if utres else []
      b = [t['arch'] for t in rvsres] if rvsres else []
      c = [t['arch'] for t in buildsres] if buildsres else []

      not_complete_archs =  [arch for arch in c if arch not in a]
      for nca in not_complete_archs:
        result = {}
        result['arch'] = nca
        result['file'] = str([res['file'] for res in buildsres if res['arch'] == nca])
        result['passed'] = PossibleUnitTestResults.UNKNOWN
        result['details'] = {}
        comp['utests'].append(result)

      comp['tests_archs'] = list(set(a+b+c))


#
# Finds for an IB the results of the static tests
#
def find_static_results( comparisons , architecture ):
  for comp in comparisons:
    rel_name = comp['compared_tags'].split('-->')[1]
    print 'Looking for static tests results for ', rel_name
    comp['static_checks'] = find_one_static_check( rel_name , architecture )
#
# Finds for an IB the results of the HLT tests
#
def find_hlt_tests_results( comparisons ):
  for comp in comparisons:
    rel_name = comp[ 'compared_tags' ].split('-->')[1]
    print 'Looking for HLT tests results for ', rel_name
    comp[ 'hlt_tests' ] = find_one_hlt_test( rel_name )

#
# Looks for one static-tests result for the IB, if it finds it, the value is 'found' if not, the value is ''
#
def find_one_static_check( release_name , architecture ):
  command_to_execute = MAGIC_COMMAND_FIND_STATIC_CHECKS.replace('RELEASE_NAME',release_name)
  command_to_execute = command_to_execute.replace('ARCHITECTURE', architecture )
  print "Running ",command_to_execute
  out,err,ret_code = get_output_command(command_to_execute)
  print "Ran:",out,err,ret_code,command_to_execute
  if ret_code == 0:
    print 'found'
    return architecture
  print 'not-found'
  return 'not-found'

#
# Looks for one hlt test result for the IB, if it finds it, the value is 'found' if not, the value is ''
#
def find_one_hlt_test( release_name ):
  command = MAGIC_COMMAND_FIND_HLT_TESTS.replace('RELEASE_NAME', release_name)
  out,err,ret_code = get_output_command(command)
  if ret_code == 0:
    print 'found'
    return 'found'
  print 'not-found'
  return 'not-found'

# reads the results and generates a separated json for each release_queue
# it also generates a csv file with statistics per release_queue and a general one
def generate_separated_json_results( results ):

  all_ibs_list = []
  all_prs_list = []

  for rq in results:
    file_name = rq['release_name'] + ".json"
    summary_file_name = rq['release_name'] + "_summary.txt"
    out_json = open(file_name, "w")
    json.dump(rq,out_json,indent=4)
    out_json.close()

    f_summary = open(summary_file_name, "w")
    ibs = [ comp['release_name'] for comp in rq['comparisons']
                                   if (comp['release_name'] != rq['base_branch']) and comp['isIB'] ]

    all_ibs_list.extend( ibs )
    
    # Ignore forward ported prs, and merge commits
    only_prs_list = []
    for comp in rq['comparisons']:
      only_prs_list.extend( [ pr['number'] for pr in comp['merged_prs'] 
                                   if not ( pr['is_merge_commit'] or pr['from_merge_commit'] ) ] )

    all_prs_list.extend( only_prs_list )
    f_summary.write( "IBs:%s\n" % ibs )
    f_summary.write( "NumIBs:%d\n" % len(ibs) )
    f_summary.write( "PRs:%s\n" % only_prs_list )
    f_summary.write( "NumPRs:%d\n" % len(only_prs_list) )
    f_summary.close()

  all_ibs_list = list( set(all_ibs_list) ) 
  all_ibs_list.sort()

  all_prs_list = list( set(all_prs_list) )
  all_prs_list.sort()

  f_summary_all = open('ibsSummaryAll.txt', "w")
  f_summary_all.write( "IBs:%s\n" % all_ibs_list )
  f_summary_all.write( "NumIBs:%d\n" % len(all_ibs_list) )

  f_summary_all.write( "PRs:%s\n" % all_prs_list )
  f_summary_all.write( "NumPRs:%d\n" % len(all_prs_list) )
  

#
# Generates a json file with the global status of the last IB for each architecture, 
# per each  Release Queue
# Schema of short_summary
# [ releaseQueue1, releaseQueue2, ... , releaseQueueN ]
# Schema of releaseQueueN
# {
#    "<ReleaseQueue>": {
#                        "<arch>": {
#                                    "status": "ok|warning|error|unknown"
#                                    "latest_IB" : "<latest IB>"
#                                  }
#                      }
# }
def generate_ib_json_short_summary( results ):
  short_summary = {}
  for rq in results:
    # this should not be called 'release name', this should be fixed
    rq_name = rq[ 'release_name' ]
    enabled_archs = RELEASES_ARCHS[ rq_name ]
    for arch in enabled_archs:
      ibs_for_current_arch  = [rel for rel in rq[ 'comparisons' ] if arch in rel[ "tests_archs" ] ]
      # it starts as ok and checks the conditions
      ib_status = 'ok'

      if len( ibs_for_current_arch ) == 0:
         latest_IB = 'N/A'
         ib_status = 'unknown'
      else:
        latest_IB_info = ibs_for_current_arch[ -1 ]
        latest_IB_name = latest_IB_info[ 'release_name' ]

        build_info = [ b for b in latest_IB_info[ "builds" ] if b[ 'arch' ] == arch ]
        if len( build_info ) == 0:
          build_passed = 'unknown'
        else: 
          build_passed = build_info[ 0 ][ "passed" ]
       
        unit_tests_info = [ u for u in latest_IB_info[ "utests" ] if u[ 'arch' ] == arch ]
        if len( unit_tests_info ) == 0:
          utests_passed = 'unknown'
        else:
          utests_passed = unit_tests_info[ 0 ][ "passed" ]

        relvals_info = [ r for r in latest_IB_info[ "relvals" ] if r[ 'arch' ] == arch ]
        if len( relvals_info ) == 0:
          relvals_passed = 'unknown'
        else:
          relvals_passed = relvals_info[ 0 ][ "passed" ]

        if not short_summary.get( rq_name ):
          short_summary[ rq_name ] = {}
        short_summary[ rq_name ][ arch ] = {}
        short_summary[ rq_name ][ arch ][ "latest_IB" ] = latest_IB_name

        merged_statuses = "%s-%s-%s" % (build_passed,utests_passed,relvals_passed)
       
        if 'unknown' in merged_statuses:
          ib_status = 'unknown'
        elif 'failed' in merged_statuses or 'False' in merged_statuses:
          ib_status = 'error'
        elif 'warning' in merged_statuses:
          ib_status = 'warning'
        short_summary[ rq_name ][ arch ][ "status" ] = ib_status

  short_summary[ 'all_archs' ] = ARCHITECTURES
  out_json = open( 'LatestIBsSummary.json' , "w" )
  json.dump( short_summary , out_json , indent=4 )
  out_json.close()

# Identifies and groups the releases accodring to their prefix
# For example if the release queues are:
# CMSSW_7_1_X, CMSSW_7_0_X, CMSSW_6_2_X, CMSSW_5_3_X, CMSSW_7_1_THREADED_X
# CMSSW_7_1_BOOSTIO_X, CMSSW_7_1_ROOT6_X, CMSSW_7_1_GEANT10_X, CMSSW_6_2_X_SLHC
# CMSSW_7_1_DEVEL_X, CMSSW_7_1_CLANG_X, CMSSW_7_2_X, CMSSW_7_2_DEVEL_X, CMSSW_7_2_CLANG_X
# CMSSW_7_2_GEANT10_X
# It will organize them like this:
# CMSSW_5_3_X: CMSSW_5_3_X
# CMSSW_7_2_X: CMSSW_7_2_X, CMSSW_7_2_DEVEL_X, CMSSW_7_2_CLANG_X, CMSSW_7_2_GEANT10_X
# CMSSW_6_2_X: CMSSW_6_2_X CMSSW_6_2_X_SLHC
# CMSSW_7_0_X: CMSSW_7_0_X
# CMSSW_7_1_X: CMSSW_7_1_X, CMSSW_7_1_THREADED_X, CMSSW_7_1_BOOSTIO_X, CMSSW_7_1_ROOT6_X',
#              CMSSW_7_1_GEANT10_X, CMSSW_7_1_DEVEL_X, CMSSW_7_1_CLANG_X
#
# It returns a dictionary in which the keys are the release prefixes, and the values are
# the release queues
def identify_release_groups(results):

  releases = [rq['release_name'] for rq in results]
  releases.sort(reverse=True)

  groups = {}

  for rel in releases:
    prefix  = rel[:10] + 'X'
    group = groups.get(prefix)
    if not group:
      groups[prefix] = []
    groups[prefix].append(rel)

  keys = groups.keys()
  keys.sort()
  groups['all_prefixes'] = keys
  groups['all_release_queues'] = releases
  return groups

#
# Using the original list of tags and the latest installed IB, it infers which IBs are currently building
# and IB is currently building if is after lastInstalledIB and incomplete compilation results are found.
# returns a list of the IBs that are currenyl being built. 
#
def getBuildingIBs( originalTags, lastInstalled ):

  print_verbose( 'last Installed: ' + lastInstalled )
  print_verbose( 'Original tags: ' + str( originalTags ) )
  print_verbose( 'index: ' + str( originalTags.index( lastInstalled ) ) )
  #these are te tags that are between the latest installed release and the head of the branch
  latestTags = originalTags[ (originalTags.index( lastInstalled )+1): ]
  print_verbose( 'latestTags: ' + str( latestTags ) )

  return [ t for t in latestTags if t in INCOMPLETE_BUILD_RESULTS.keys() ]
  

#-----------------------------------------------------------------------------------
#---- Start of execution
#-----------------------------------------------------------------------------------

if __name__ == "__main__":

  MAGIC_COMMAND_CMSDIST_TAGS = "pushd %s; git tag -l */*/ARCHITECTURE | grep -E 'IB|ERR'; popd" % CMSDIST_REPO
  CMSSDT_DIR = "/data/sdt"
  BUILD_LOG_DIR = CMSSDT_DIR+"/buildlogs"
  AFS_LOG_DIR = "/afs/cern.ch/cms/sw/ReleaseCandidates"
  JENKINS_ARTIFACTS_SUBDIR = "SDT/jenkins-artifacts"
  JENKINS_ARTIFACTS_DIR = CMSSDT_DIR+"/"+JENKINS_ARTIFACTS_SUBDIR
  # I used this type of concatenation because the string has %s inside
  MAGIC_COMMAND_PRS  = 'GIT_DIR='+CMSSW_REPO+' git log --merges  --pretty=\'"%H,%s", "%b", "tags->,%d"\' START_TAG..END_TAG'
  MAGIC_COMMAND_FIND_FIRST_MERGE_WITH_TAG = 'GIT_DIR='+CMSSW_REPO+' git log --merges  --pretty=\'"%s", "%b", "tags->,%d"\' END_TAG | grep "tags->" | head -n1'
  MAGIC_COMMAD_FIND_RESULTS_RELVALS  = 'find '+BUILD_LOG_DIR+'/ARCHITECTURE/www  -mindepth 6 -maxdepth 6 -path "*/pyRelValMatrixLogs/run/runall-report-step123-.log"'
  MAGIC_COMMAD_FIND_INCOMPLETE_RESULTS_RELVALS = "find cms-sw.github.io/data/relvals/ -name '*INCOMPLETE.json'"
  MAGIC_COMMAND_FIND_EXCEPTIONS_RESULTS_RELVALS ="find cms-sw.github.io/data/relvals/ -name '*EXCEPTIONS.json'"
  MAGIC_COMMAND_TAGS = 'GIT_DIR='+CMSSW_REPO+' git log --merges  --pretty=\'"%s", "%b", "tags->,%d"\' START_TAG..END_TAG | grep -E "Merge "'
  MAGIC_COMMAND_FIND_RESULTS_UNIT_TESTS = 'find '+BUILD_LOG_DIR+'/ARCHITECTURE/www -mindepth 4 -maxdepth 4 -name unitTests-summary.log'
  MAGIC_COMMAND_FIND_RESULTS_ADDON  = 'find '+BUILD_LOG_DIR+'/ARCHITECTURE/www -mindepth 4 -maxdepth 4 -name addOnTests.log'
  MAGIC_COMMAND_FIND_RESULTS_BUILD = 'find '+BUILD_LOG_DIR+'/ARCHITECTURE/www  -mindepth 5 -maxdepth 5 -name logAnalysis.pkl | grep -v /new_FWLITE/'
  MAGIC_COMMAD_FIND_INCOMPLETE_RESULTS_BUILD = "find cms-sw.github.io/data/scram/ -name '*INCOMPLETE.json'"
  STATIC_CHECKS_URL = 'http://' + ARTIFACTS_MACHINE + '/'+JENKINS_ARTIFACTS_SUBDIR+'/ib-static-analysis/RELEASE_NAME/ARCHITECTURE/llvm-analysis/index.html'
  HLT_TESTS_URL = 'http://' + ARTIFACTS_MACHINE + '/'+JENKINS_ARTIFACTS_SUBDIR+'/HLT-Validation/RELEASE_NAME/'
  MAGIC_COMMAND_FIND_STATIC_CHECKS = 'test -f '+JENKINS_ARTIFACTS_DIR+'/ib-static-analysis/RELEASE_NAME/ARCHITECTURE/llvm-analysis/index.html'
  MAGIC_COMMAND_FIND_HLT_TESTS = 'test -d '+JENKINS_ARTIFACTS_DIR+'/HLT-Validation/RELEASE_NAME'
  CONFIG_MAP_FILE = 'config.map'
  # this will be filled using config.map by get_config_map_params()
  ARCHITECTURES = []
  # this will be filled using config.map by get_config_map_params()
  RELEASES_BRANCHES = {}
  # this will be filled using config.map by get_config_map_params() SLHC releases have a different format, so it is hardcoded
  SPECIAL_RELEASES = [ 'SLHC' ]
  # this will be filled using config.map by get_config_map_params()
  SP_REL_REGEX = ""
  # These are the release queues that need to be shown, this this will be filled using config.map by get_config_map_params()
  RELEASE_QUEUES = []
  # These are the ibs and archs for which the aditional tests need to be shown
  # The schema is:
  # {
  #   "<IBName>": {
  #                 "<architecture>" : [ test1, test2, ... , testN ]
  #               }
  # }
  # This this will be filled using config.map by get_config_map_params()
  RELEASE_ADITIONAL_TESTS = {}
  # the acrhitectures for which the enabled releases are currently avaiable
  # The schema is:
  # {
  #   "<IBName>": [ "arch1" , "arch2" , ... ,"archN" ]
  # }
  # will be filled using config.map by get_config_map_params()
  RELEASES_ARCHS = {}
  # The IBs and arch for which incomplete results are availavle
  # this is loaded earlier to allow to discard tags since the beginning
  # The schema is:
  # {
  #   "<IBName>": [ "arch1" , "arch2" , ... ,"archN" ]
  # }
  # will be filled with fill_list_incomplete_results( type )
  INCOMPLETE_BUILD_RESULTS = {}
  # The IBs and arch for which relval results are availavle
  # The schema is:
  # {
  #   "<IBName>": [ "arch1" , "arch2" , ... ,"archN" ]
  # }
  # will be filled with fill_list_incomplete_results( type )
  INCOMPLETE_RELVALS_RESULTS = {}  
  # A list of IBs which are being built
  # will be filled with getBuildingIBs( originalTags, lastInstalled )
  ALL_BUILDING_IBS = []
  MAGIC_COMMAND_FIND_ALL_TAGS ='GIT_DIR='+CMSSW_REPO+' git log --merges  --pretty=\'"%s", "%b", "tags->,%d"\' END_TAG | grep -E "Merge " | grep -E "RELEASE_QUEUE"'
  # This regular expression allows to identify if a merge commit is an automatic forward port
  AUTO_FORWARD_PORT_REGEX='^.*Merge CMSSW.+ into CMSSW.+$'


  class BuildResultsKeys:
    DICT_ERROR = 'dictError'
    COMP_ERROR = 'compError'
    LINK_ERROR = 'linkError'
    COMP_WARNING = 'compWarning'
    DWNL_ERROR = 'dwnlError'
    MISC_ERROR = 'miscError'
    IGNORE_WARNING = 'ignoreWarning'
    PYTHON_ERROR = 'pythonError'

  class PossibleBuildResults:
    PASSED = 'passed'
    WARNING = 'warning'
    ERROR = 'error'

  class PossibleUnitTestResults:
    PASSED = 'passed'
    FAILED = 'failed'
    UNKNOWN = 'unknown'
 
  results = []

  get_config_map_params()
  SP_REL_REGEX = "|".join( SPECIAL_RELEASES )
  REQUESTED_COMPARISONS = [ ( '%s_%s..%s' % ( rq , START_DATE , rq ) ) for rq in RELEASE_QUEUES ]

  AFS_IB_INSTALLATION = "/afs/cern.ch/cms/sw/ReleaseCandidates/vol*/*/cms"
  AFS_INSTALLATION = "/afs/cern.ch/cms/*/cms"
  installedPaths = [x for x in glob(AFS_IB_INSTALLATION + "/cmssw/*")]
  installedPaths += [x for x in glob(AFS_IB_INSTALLATION + "/cmssw-patch/*")]
  installedPaths += [x for x in glob(AFS_INSTALLATION + "/cmssw/*")]
  installedPaths += [x for x in glob(AFS_INSTALLATION + "/cmssw-patch/*")]

  installedReleases = [basename(x) for x in installedPaths]

  print_verbose( 'Installed Releases:' )
  print_verbose( installedReleases )

  fill_list_incomplete_results( 'builds' )
  fill_list_incomplete_results( 'relvals' )

  for comp in REQUESTED_COMPARISONS:
    start_tag = comp.split("..")[0]
    end_tag = comp.split("..")[1]
    release_queue = start_tag
    # if is a SLHC or any special release, the split will happen with the fifth underscore _
    if re.search(SP_REL_REGEX,release_queue):
      print_verbose( 'This is a special release' )
      release_queue = re.match(r'^((?:[^_]*_){%d}[^_]*)_(.*)' % (4), release_queue).groups()[0]
    else:
      release_queue = re.match(r'^((?:[^_]*_){%d}[^_]*)_(.*)' % (3), release_queue).groups()[0]
  
    print '####################################################################'
    print "I will analyze %s from %s to %s:" % (release_queue,start_tag,end_tag)

    # load the graph to check the history and identify merge prs and commits
    release_branch = RELEASES_BRANCHES[ release_queue ]
    print 'Identifying merge commits...'
    graph = load_graph( release_branch , -1 )
    print 'Done'

    release_queue_results = {}

    release_queue_results[ 'release_name' ] = release_queue
    release_queue_results[ 'base_branch' ] = release_branch

    print 'Identifying tags...'
    tags = execute_magic_command_tags( start_tag, end_tag, release_queue, release_branch )
    originalTags = tags
    tags = [x for x in tags if x in installedReleases]
    # this is to get the IBs that are not installed and most probably are being built
    if len( tags ) > 0:
      lastInstalled = tags[ -1 ]
      buildingIBs = getBuildingIBs( originalTags, lastInstalled )
      ALL_BUILDING_IBS.extend( buildingIBs )
      tags.extend( buildingIBs ) 
 
    tags.append( release_branch )
    print 'I got these tags: '
    print tags 

    print 'Getting merged pull requests between tags...'
    release_queue_results['comparisons'] = compare_tags( tags , graph )
    print 'Done'

    #it checks if the tests are being run for that architectue, if they don't, it doesn't look for them
    additional_tests = RELEASE_ADITIONAL_TESTS.get( release_queue )
    if additional_tests:
      for arch in additional_tests.keys():
        tests_to_find = additional_tests[ arch ]
        if 'HLT' in tests_to_find:
          find_hlt_tests_results( release_queue_results['comparisons'] )
        if 'static-checks' in tests_to_find:
          find_static_results( release_queue_results['comparisons'] , arch )
  
    results.append(release_queue_results)
  
  
  cmsdist_tags_results = execute_magic_command_get_cmsdist_tags()
  
  build_results = {}
  execute_magic_command_find_results( build_results , 'builds' )
  execute_magic_command_find_incomplete_results( build_results, 'builds' )
  
  relvals_results = {}
  execute_magic_command_find_results( relvals_results , 'relvals' )
  execute_magic_command_find_incomplete_results( relvals_results, 'relvals' )
  
  unit_tests_results = {}
  execute_magic_command_find_results( unit_tests_results , 'utests' )
  
  addOn_tests_results = {}
  execute_magic_command_find_results( addOn_tests_results ,'addOn' )

  rv_Exceptions_Results = execute_magic_command_find_rv_exceptions_results() 
 
  add_tests_to_results( results, unit_tests_results, relvals_results, 
                       addOn_tests_results, build_results, cmsdist_tags_results,
                       rv_Exceptions_Results )

  fill_missing_cmsdist_tags( results )
  print_results(results)
  
  structure = identify_release_groups( results )
  generate_separated_json_results( results )
  generate_ib_json_short_summary( results )
  
  out_json = open("merged_prs_summary.json", "w")
  json.dump(results,out_json,indent=4)
  out_json.close()

  out_groups = open("structure.json", "w")
  json.dump(structure,out_groups,indent=4)
  out_groups.close()

